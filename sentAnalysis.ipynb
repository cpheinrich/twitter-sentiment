{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'text']\n",
      "5250\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "home_dir = os.getcwd()\n",
    "fname = os.path.join(home_dir,'data/train_data.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "lines = lines[:-1]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76, 473, 3287, 1062, 352]\n",
      "5250\n",
      "5250\n",
      "A sample tweet: Autonomous vehicles could reduce traffic fatalities by 90%...I'm in!\n",
      "Has sentiment: 4\n"
     ]
    }
   ],
   "source": [
    "sents = [0,0,0,0,0]\n",
    "labels = []\n",
    "texts = []\n",
    "for line in lines:\n",
    "    sent = int(line.split(',')[0])\n",
    "    tweet = line.split(',')[1]\n",
    "    sents[sent -1] +=1\n",
    "    texts.append(tweet)\n",
    "    labels.append(sent-1) #data is labeled 1-5, so we shift by 1 so it starts with 0\n",
    "    \n",
    "print(sents)\n",
    "print(len(labels))\n",
    "print(len(texts))\n",
    "\n",
    "print(\"A sample tweet: \" + texts[6] )\n",
    "print(\"Has sentiment: \" + str(labels[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data\n",
    "\n",
    "We first vectorize the data we collected and prepare a training and validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10982 unique tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 20 # We cut the tweet after 20 words (most are shorter than this anyway)\n",
    "training_samples = 4000\n",
    "validation_samples = len(labels) - training_samples\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "x_train = sequences[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = sequences[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(x_train)\n",
    "# Our vectorized test data\n",
    "x_val = vectorize_sequences(x_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We one-hot encode the lables using a Keras convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train,)\n",
    "y_val = to_categorical(y_val,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(5, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 320,645\n",
      "Trainable params: 320,645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1250 samples\n",
      "Epoch 1/30\n",
      "4000/4000 [==============================] - 1s 200us/step - loss: 1.4437 - acc: 0.5777 - val_loss: 1.2557 - val_acc: 0.5936\n",
      "Epoch 2/30\n",
      "4000/4000 [==============================] - 0s 125us/step - loss: 1.1746 - acc: 0.6360 - val_loss: 1.1520 - val_acc: 0.5936\n",
      "Epoch 3/30\n",
      "4000/4000 [==============================] - 0s 123us/step - loss: 1.0902 - acc: 0.6360 - val_loss: 1.1146 - val_acc: 0.5936\n",
      "Epoch 4/30\n",
      "4000/4000 [==============================] - 1s 126us/step - loss: 1.0312 - acc: 0.6372 - val_loss: 1.0871 - val_acc: 0.5936\n",
      "Epoch 5/30\n",
      "4000/4000 [==============================] - 0s 125us/step - loss: 0.9753 - acc: 0.6440 - val_loss: 1.0629 - val_acc: 0.5968\n",
      "Epoch 6/30\n",
      "4000/4000 [==============================] - 0s 124us/step - loss: 0.9092 - acc: 0.6555 - val_loss: 1.0470 - val_acc: 0.6240\n",
      "Epoch 7/30\n",
      "4000/4000 [==============================] - 0s 120us/step - loss: 0.8699 - acc: 0.6715 - val_loss: 1.0407 - val_acc: 0.6264\n",
      "Epoch 8/30\n",
      "4000/4000 [==============================] - 0s 123us/step - loss: 0.8208 - acc: 0.6885 - val_loss: 1.0442 - val_acc: 0.6280\n",
      "Epoch 9/30\n",
      "4000/4000 [==============================] - 1s 129us/step - loss: 0.7725 - acc: 0.7115 - val_loss: 1.0481 - val_acc: 0.6128\n",
      "Epoch 10/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.7284 - acc: 0.7335 - val_loss: 1.0455 - val_acc: 0.6192\n",
      "Epoch 11/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.6816 - acc: 0.7542 - val_loss: 1.0535 - val_acc: 0.6120\n",
      "Epoch 12/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.6424 - acc: 0.7725 - val_loss: 1.0706 - val_acc: 0.6080\n",
      "Epoch 13/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.6004 - acc: 0.7823 - val_loss: 1.0902 - val_acc: 0.6096\n",
      "Epoch 14/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.5672 - acc: 0.8023 - val_loss: 1.1169 - val_acc: 0.5848\n",
      "Epoch 15/30\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 0.5323 - acc: 0.8113 - val_loss: 1.1443 - val_acc: 0.5880\n",
      "Epoch 16/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.5046 - acc: 0.8233 - val_loss: 1.1729 - val_acc: 0.5760\n",
      "Epoch 17/30\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 0.4672 - acc: 0.8372 - val_loss: 1.2081 - val_acc: 0.5912\n",
      "Epoch 18/30\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 0.4441 - acc: 0.8450 - val_loss: 1.2602 - val_acc: 0.5992\n",
      "Epoch 19/30\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 0.4227 - acc: 0.8505 - val_loss: 1.2723 - val_acc: 0.5952\n",
      "Epoch 20/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.3943 - acc: 0.8653 - val_loss: 1.3060 - val_acc: 0.5712\n",
      "Epoch 21/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.3737 - acc: 0.8705 - val_loss: 1.3429 - val_acc: 0.5792\n",
      "Epoch 22/30\n",
      "4000/4000 [==============================] - 0s 123us/step - loss: 0.3570 - acc: 0.8795 - val_loss: 1.3990 - val_acc: 0.5752\n",
      "Epoch 23/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.3399 - acc: 0.8905 - val_loss: 1.4467 - val_acc: 0.5848\n",
      "Epoch 24/30\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 0.3190 - acc: 0.8952 - val_loss: 1.4734 - val_acc: 0.5760\n",
      "Epoch 25/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.3042 - acc: 0.9038 - val_loss: 1.5163 - val_acc: 0.5648\n",
      "Epoch 26/30\n",
      "4000/4000 [==============================] - 0s 122us/step - loss: 0.2900 - acc: 0.9085 - val_loss: 1.5644 - val_acc: 0.5824\n",
      "Epoch 27/30\n",
      "4000/4000 [==============================] - 0s 121us/step - loss: 0.2729 - acc: 0.9133 - val_loss: 1.6342 - val_acc: 0.5856\n",
      "Epoch 28/30\n",
      "4000/4000 [==============================] - 0s 120us/step - loss: 0.2569 - acc: 0.9225 - val_loss: 1.6341 - val_acc: 0.5504\n",
      "Epoch 29/30\n",
      "4000/4000 [==============================] - 1s 126us/step - loss: 0.2556 - acc: 0.9220 - val_loss: 1.7146 - val_acc: 0.5872\n",
      "Epoch 30/30\n",
      "4000/4000 [==============================] - 1s 126us/step - loss: 0.2312 - acc: 0.9303 - val_loss: 1.7387 - val_acc: 0.5720\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs = 30, batch_size=128,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
