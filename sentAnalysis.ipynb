{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrisheinrich/anaconda/envs/deep/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/chrisheinrich/anaconda/envs/deep/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'text']\n",
      "5250\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "home_dir = os.getcwd()\n",
    "fname = os.path.join(home_dir,'data/train_data.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]\n",
    "lines = lines[:-1]\n",
    "\n",
    "print(header)\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76, 473, 3287, 1062, 352]\n",
      "5250\n",
      "5250\n",
      "A sample tweet: Autonomous vehicles could reduce traffic fatalities by 90%...I'm in!\n",
      "Has sentiment: 4\n"
     ]
    }
   ],
   "source": [
    "sents = [0,0,0,0,0]\n",
    "labels = []\n",
    "texts = []\n",
    "for line in lines:\n",
    "    sent = int(line.split(',')[0])\n",
    "    tweet = line.split(',')[1]\n",
    "    sents[sent -1] +=1\n",
    "    texts.append(tweet)\n",
    "    labels.append(sent-1) #data is labeled 1-5, so we shift by 1 so it starts with 0\n",
    "    \n",
    "print(sents)\n",
    "print(len(labels))\n",
    "print(len(texts))\n",
    "\n",
    "print(\"A sample tweet: \" + texts[6] )\n",
    "print(\"Has sentiment: \" + str(labels[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data\n",
    "\n",
    "We first vectorize the data we collected and prepare a training and validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10982 unique tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 20 # We cut the tweet after 20 words (most are shorter than this anyway)\n",
    "training_samples = 4000\n",
    "validation_samples = len(labels) - training_samples\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "x_train = sequences[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = sequences[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(x_train)\n",
    "# Our vectorized test data\n",
    "x_val = vectorize_sequences(x_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We one-hot encode the lables using a Keras convenience function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train,)\n",
    "y_val = to_categorical(y_val,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "#model.add(layers.Dense(32, activation='relu'))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(5, activation='softmax')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 160,101\n",
      "Trainable params: 160,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1250 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 1s 147us/step - loss: 1.3336 - acc: 0.6168 - val_loss: 1.2152 - val_acc: 0.5936\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 0s 88us/step - loss: 1.1144 - acc: 0.6362 - val_loss: 1.1489 - val_acc: 0.5936\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 1.0357 - acc: 0.6370 - val_loss: 1.1106 - val_acc: 0.5992\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 0s 87us/step - loss: 0.9660 - acc: 0.6445 - val_loss: 1.0764 - val_acc: 0.6136\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 0.8973 - acc: 0.6637 - val_loss: 1.0563 - val_acc: 0.6272\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 0.8343 - acc: 0.6827 - val_loss: 1.0336 - val_acc: 0.6272\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 0.7768 - acc: 0.7037 - val_loss: 1.0276 - val_acc: 0.6224\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 0s 85us/step - loss: 0.7225 - acc: 0.7318 - val_loss: 1.0295 - val_acc: 0.6200\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 0s 103us/step - loss: 0.6732 - acc: 0.7535 - val_loss: 1.0170 - val_acc: 0.6200\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 0.6282 - acc: 0.7698 - val_loss: 1.0364 - val_acc: 0.6248\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 0.5851 - acc: 0.8007 - val_loss: 1.0380 - val_acc: 0.6224\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 0.5465 - acc: 0.8180 - val_loss: 1.0369 - val_acc: 0.6176\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 0.5101 - acc: 0.8323 - val_loss: 1.0362 - val_acc: 0.6184\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 0.4758 - acc: 0.8468 - val_loss: 1.0524 - val_acc: 0.6080\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 0.4438 - acc: 0.8608 - val_loss: 1.0619 - val_acc: 0.6080\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 0.4144 - acc: 0.8715 - val_loss: 1.0775 - val_acc: 0.5944\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 0s 113us/step - loss: 0.3866 - acc: 0.8810 - val_loss: 1.0863 - val_acc: 0.5944\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 0.3615 - acc: 0.8890 - val_loss: 1.1096 - val_acc: 0.5872\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 0s 112us/step - loss: 0.3376 - acc: 0.8980 - val_loss: 1.1160 - val_acc: 0.5944\n",
      "Epoch 20/20\n",
      "4000/4000 [==============================] - 0s 111us/step - loss: 0.3160 - acc: 0.9055 - val_loss: 1.1257 - val_acc: 0.5968\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs = 20, batch_size=128,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x183f4aef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looks like we start over fitting around 6 epochs, reaching a maximum accuracy of 63%. This is not so good considering this is accuracy you would get if you just guessed a neutral sentiment(3) for every tweet.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casting as a binary classification problem\n",
    "\n",
    "One of the reasons this classification is challenging is that there are 5 classes, and the majority of the sentiments are neutral (i.e. 3). I am interested to see what the accuracy is like if we instead cast this as a binary classification problem. We will do this by throwing away all of the neutral reviews, setting the negative reviews to 0 and positive reviews to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963\n",
      "1963\n",
      "A sample tweet: The future! So buying one of these. ̢���@CNET: Audi is ready to test autonomous cars on public roads http://t.co/aZTbHLcy #CES #2013CES̢����\n",
      "Has sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "for line in lines:\n",
    "    sent = int(line.split(',')[0])\n",
    "    tweet = line.split(',')[1]\n",
    "    if sent < 3:\n",
    "        texts.append(tweet)\n",
    "        labels.append(0)\n",
    "    if sent > 3:\n",
    "        texts.append(tweet)\n",
    "        labels.append(1)\n",
    "\n",
    "print(len(labels))\n",
    "print(len(texts))\n",
    "\n",
    "print(\"A sample tweet: \" + texts[6] )\n",
    "print(\"Has sentiment: \" + str(labels[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5192 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 20 # We cut the tweet after 20 words (most are shorter than this anyway)\n",
    "training_samples = 1400\n",
    "validation_samples = len(labels) - training_samples\n",
    "max_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "x_train = sequences[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "\n",
    "x_val = sequences[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "y_train[0]\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1  # set specific indices of results[i] to 1s\n",
    "    return results\n",
    "\n",
    "\n",
    "# Our vectorized training data\n",
    "x_train = vectorize_sequences(x_train)\n",
    "# Our vectorized test data\n",
    "x_val = vectorize_sequences(x_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our vectorized labels\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_test = np.asarray(y_val).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "#model.add(layers.Dense(32, activation='relu'))\n",
    "#model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1, activation='sigmoid')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_24 (Dense)             (None, 16)                160016    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 563 samples\n",
      "Epoch 1/20\n",
      "1400/1400 [==============================] - 1s 366us/step - loss: 0.6695 - acc: 0.6557 - val_loss: 0.6387 - val_acc: 0.7531\n",
      "Epoch 2/20\n",
      "1400/1400 [==============================] - 0s 120us/step - loss: 0.6235 - acc: 0.7114 - val_loss: 0.6118 - val_acc: 0.7513\n",
      "Epoch 3/20\n",
      "1400/1400 [==============================] - 0s 114us/step - loss: 0.5894 - acc: 0.7121 - val_loss: 0.5921 - val_acc: 0.7513\n",
      "Epoch 4/20\n",
      "1400/1400 [==============================] - 0s 125us/step - loss: 0.5578 - acc: 0.7243 - val_loss: 0.5777 - val_acc: 0.7513\n",
      "Epoch 5/20\n",
      "1400/1400 [==============================] - 0s 119us/step - loss: 0.5271 - acc: 0.7486 - val_loss: 0.5663 - val_acc: 0.7531\n",
      "Epoch 6/20\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 0.4969 - acc: 0.7807 - val_loss: 0.5575 - val_acc: 0.7531\n",
      "Epoch 7/20\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 0.4670 - acc: 0.8014 - val_loss: 0.5522 - val_acc: 0.7584\n",
      "Epoch 8/20\n",
      "1400/1400 [==============================] - 0s 117us/step - loss: 0.4379 - acc: 0.8357 - val_loss: 0.5448 - val_acc: 0.7584\n",
      "Epoch 9/20\n",
      "1400/1400 [==============================] - 0s 114us/step - loss: 0.4102 - acc: 0.8457 - val_loss: 0.5438 - val_acc: 0.7602\n",
      "Epoch 10/20\n",
      "1400/1400 [==============================] - 0s 113us/step - loss: 0.3840 - acc: 0.8643 - val_loss: 0.5437 - val_acc: 0.7478\n",
      "Epoch 11/20\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 0.3590 - acc: 0.8836 - val_loss: 0.5405 - val_acc: 0.7460\n",
      "Epoch 12/20\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 0.3359 - acc: 0.8950 - val_loss: 0.5398 - val_acc: 0.7478\n",
      "Epoch 13/20\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 0.3142 - acc: 0.9057 - val_loss: 0.5421 - val_acc: 0.7496\n",
      "Epoch 14/20\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 0.2939 - acc: 0.9193 - val_loss: 0.5431 - val_acc: 0.7513\n",
      "Epoch 15/20\n",
      "1400/1400 [==============================] - 0s 115us/step - loss: 0.2756 - acc: 0.9257 - val_loss: 0.5441 - val_acc: 0.7460\n",
      "Epoch 16/20\n",
      "1400/1400 [==============================] - 0s 114us/step - loss: 0.2577 - acc: 0.9393 - val_loss: 0.5455 - val_acc: 0.7389\n",
      "Epoch 17/20\n",
      "1400/1400 [==============================] - 0s 116us/step - loss: 0.2415 - acc: 0.9450 - val_loss: 0.5439 - val_acc: 0.7371\n",
      "Epoch 18/20\n",
      "1400/1400 [==============================] - 0s 117us/step - loss: 0.2264 - acc: 0.9471 - val_loss: 0.5493 - val_acc: 0.7336\n",
      "Epoch 19/20\n",
      "1400/1400 [==============================] - 0s 132us/step - loss: 0.2122 - acc: 0.9543 - val_loss: 0.5541 - val_acc: 0.7318\n",
      "Epoch 20/20\n",
      "1400/1400 [==============================] - 0s 117us/step - loss: 0.1988 - acc: 0.9593 - val_loss: 0.5542 - val_acc: 0.7336\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,epochs = 20, batch_size=128,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us to a max accuracy of around 76%. However, considering this dataset isn't balanced (about 72% are positive and 28% negative), we are actually only doing slightly better than if we had just guesed positive for all of the reviews. One plausible explanation here is that their just isn't much training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
